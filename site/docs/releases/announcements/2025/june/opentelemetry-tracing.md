---
title: Tracing - See Inside Your LLM Applications with OpenTelemetry
sidebar_label: OpenTelemetry Tracing
description: OpenTelemetry tracing support to visualize execution flow, measure performance, and debug LLM applications
date: June 15, 2025
---

# Tracing - See Inside Your LLM Applications with OpenTelemetry

We've added [OpenTelemetry tracing support](/docs/tracing/) to help you understand what's happening inside your AI applications. Previously, LLM applications were often "black boxes"â€”you could see inputs and outputs, but not what happened in between. Now you can visualize the entire execution flow, measure performance of individual steps, and quickly identify issues.

![Tracing and OpenTelemetry Support](/img/docs/trace.png)

This is especially valuable for complex RAG pipelines or multi-step workflows where you need to identify performance bottlenecks or debug failures.

**Use it when:**

- Debugging slow RAG pipelines
- Optimizing multi-step agent workflows
- Understanding why certain requests fail
- Measuring performance across different providers

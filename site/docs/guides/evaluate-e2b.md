---
title: Evaluate e2b Sandbox
sidebar_label: Evaluate e2b
description: Hands-on tutorial (September 2025) on evaluating LLM code generation using Promptfoo with the e2b sandbox—includes setup, YAML tests, execution in a secure environment, and metrics reporting.
keywords:
  [
    e2b evaluation,
    e2b sandbox,
    Promptfoo tests,
    LLM code execution,
    code security,
    code generation evaluation,
  ]
---

# Evaluate e2b: Secure Code Generation and Execution

[E2B](https://github.com/e2b-dev/E2B) is an open-source infrastructure that provides a secure, isolated sandbox environment in the cloud for running untrusted code—such as code generated by LLMs. Instead of executing model outputs directly on your local machine or inside Docker, E2B sandboxes allow safe evaluation of Python or JavaScript code in isolation, controlled via a simple SDK and API.

With **Promptfoo**, you can integrate e2b into your evaluation pipeline:

- Define prompts for code generation.
- Automatically run the generated code in e2b.
- Validate outputs against test cases.
- Collect metrics (runtime, success/failure, logs).

By the end you’ll safely run LLM-generated code in a sandbox and view results in Promptfoo.

## How the flow works (quick overview)

- **Promptfoo** sends prompts to the selected **LLM**.
- The **LLM** generates Python function code.
- Promptfoo runs `validate_and_run_code_e2b.py` to extract and safety-check the code.
- The code executes inside the **E2B sandbox** with strict limits.
- **Promptfoo** collects results and metrics, showing them in the CLI or web viewer.

## Highlights

- Setting up the project directory
- Installing Promptfoo and e2b dependencies
- Writing prompts for function/code generation
- Running generated code in the sandbox with safety checks
- Configuring test cases in YAML
- Collecting metrics and generating reports

To scaffold the **e2b + Promptfoo example**, you can run:

```bash
npx promptfoo@latest init --example e2b-code-eval
```

## Requirements

Before starting, make sure you have:

- Python 3.9–3.12 (tested)
- Node.js v22 LTS or newer
- OpenAI API key (for code generation)
- e2b API key (for sandbox execution)

---

## Step 1: Initial Setup

Check that your environment is ready.

**Python installed**

```bash
python3 --version
```

You should see Python 3.10.x or newer, you're good to go.

**Node.js + npm installed**

```bash
node -v
npm -v
```

Expected:

- Node.js v22.x.x (LTS)
- npm 10.x.x

You should see something like v22.x.x for Node and 10.x.x for npm. Node.js v22 LTS or newer is recommended for security and performance.

Why do we need these?

- Python is used to run evaluation scripts and helpers.
- Node.js + npm are required to install and run [Promptfoo CLI](https://www.promptfoo.dev/docs/usage/command-line/), which drives the evaluation pipeline.
- Together, they allow you to generate code, send it to e2b for execution, and collect results safely.

If you're missing any of these, install them first before moving on.

## Step 2: Create Your Project Folder

Run these commands in your terminal:

```bash
mkdir e2b-code-eval
cd e2b-code-eval
```

What's happening here?

- `mkdir e2b-code-eval`: Makes a fresh directory called e2b-code-eval.
- `cd e2b-code-eval`: Moves you into that directory.

## Step 3: Configure Environment Variables

Before running any evaluation, you need to set up your API keys. These tell Promptfoo how to talk to both **e2b** (for sandboxed execution) and **OpenAI** (for code generation).

Set these in your shell:

```bash
# Required
export E2B_API_KEY="e2b_xxx_your_key_here"       # e2b sandbox API key
export OPENAI_API_KEY="sk_xxx_your_key_here"     # OpenAI key (or your chosen LLM provider)

# Recommended
export PROMPTFOO_PYTHON="$(pwd)/.venv/bin/python"  # ensure promptfoo uses your venv Python
```

`E2B_API_KEY`: Authenticates your sandbox sessions with e2b.
`OPENAI_API_KEY`: Lets Promptfoo query OpenAI models for code generation.
`PROMPTFOO_PYTHON`: Ensures Promptfoo evaluations run inside the same virtual environment you’re using for Python.

If you configure a different provider in your promptfooconfig.yaml, export that provider’s API key instead of OPENAI_API_KEY.

## Step 4: Set Up Virtual Environment and Install Dependencies

To keep things clean, it’s best to use a Python virtual environment (venv) for your project.

Create and activate a virtual environment:

```bash
python -m venv .venv
source .venv/bin/activate
```

Now upgrade pip and install the required Python packages:

```bash
pip install --upgrade pip
pip install e2b-code-interpreter python-dotenv
```

Finally, install Promptfoo globally via npm:

```bash
npm install -g promptfoo
```

## Verify setup

Run these checks to make sure everything is ready:

```bash
python3 -c "import e2b_code_interpreter, dotenv; print('✅ Python libs ready')"
npx promptfoo --version
```

If you see a ✅ message from Python and a version number from Promptfoo, your environment is fully set up.

## Step 5: Initialize Promptfoo Project

With your environment ready, the next step is to scaffold a **Promptfoo evaluation project**. This sets up the config and folder structure you’ll use for running tests.

In your project folder, run:

```bash
npx promptfoo init
```

- Pick Not sure yet to scaffold basic config.
- Select OpenAI as the model provider.

At the end, you get:

- README.md
- promptfooconfig.yaml

## Step 6: Write `validate_and_run_code_e2b.py`, `metrics.py`, `report.py`, `swe_runner.py`, `promptfooconfig.yaml` and `Prompt Files`

- In this step, we’ll define all the files needed for evaluating LLM-generated code securely with e2b sandbox.
- This includes validation logic, metrics logging, report generation, YAML config, and prompt templates.

### Create `validate_and_run_code_e2b.py`

The main evaluation logic lives in [`examples/e2b-code-eval/validate_and_run_code_e2b.py`](https://github.com/promptfoo/promptfoo/blob/main/examples/e2b-code-eval/validate_and_run_code_e2b.py).

This script:

- Extracts Python functions from LLM output (via regex or fenced code blocks).
- Runs static safety checks to block dangerous code before execution.
- Executes generated code inside an E2B sandbox with strict limits.
- Normalizes results (stdout, stderr, logs) for analysis.
- Records metrics and returns structured pass/fail results to Promptfoo.

### Create `metrics.py`

The metrics helper lives in [`examples/e2b-code-eval/metrics.py`](https://github.com/promptfoo/promptfoo/blob/main/examples/e2b-code-eval/metrics.py).

This script:

- Defines `write_metrics`, a helper function to save evaluation results.
- Collects metadata like task ID, provider, model, runtime, success/failure.
- Supports adding debug info (logs, error messages, etc.).
- Saves results as structured JSON files in `.promptfoo_results/`.

### Create `report.py`

The reporting script lives in [`examples/e2b-code-eval/report.py`](https://github.com/promptfoo/promptfoo/blob/main/examples/e2b-code-eval/report.py).

This script:

- Reads all JSON metric files generated by `metrics.py`.
- Parses results and summarizes them in a Markdown table.
- Shows task, provider, model, success, and runtime.
- Writes the output to `promptfoo_report.md` for human-readable reporting.

### Create `swe_runner.py`

The SWE-bench–style runner lives in [`examples/e2b-code-eval/swe_runner.py`](https://github.com/promptfoo/promptfoo/blob/main/examples/e2b-code-eval/swe_runner.py).

This script:

- Clones a target repo into a temp dir and captures failing test output.
- Reads relevant files and calls model_call_patch (you implement) to obtain a unified diff.
- Applies the patch locally with git apply.
- Executes the test command inside an e2b sandbox to verify the fix.
- Cleans up temp directories and reports success when return code is 0.

### Edit `promptfooconfig.yaml`

The evaluation config lives in [`examples/e2b-code-eval/promptfooconfig.yaml`](https://github.com/promptfoo/promptfoo/blob/main/examples/e2b-code-eval/promptfooconfig.yaml).

This file:

- Points prompts: to code_generation_prompt_fs.txt.
- Selects an LLM provider (e.g., openai:gpt-4.1).
- Defines test cases (problem, function_name, test_input, expected_output).
- Sets the default assertion to `file://validate_and_run_code_e2b.py`, which runs the sandboxed check.

### Prompt Templates

All prompt templates are in [`examples/e2b-code-eval/`](https://github.com/promptfoo/promptfoo/tree/main/examples/e2b-code-eval)

`code_generation_prompt_fs.txt`
Instructs the model to output only a Python function definition wrapped in triple backticks, using the exact {`function_name`}, avoiding network/filesystem access, and raising ValueError for clearly invalid inputs.

`generate_unit_test_prompt.txt`
Asks the model to produce 3 concise assert tests (no frameworks) that cover typical and edge cases for a provided function body.

`patch_generation_prompt.txt`
Directs the model to return a minimal unified diff (git-apply ready) that fixes a failing test using the supplied fail log and relevant file contents — no prose, just the diff.

## Step 7: Run Your First Evaluation

Now that everything is set up, it's time to run your first evaluation with e2b sandbox integration.

Run the evaluation:

```bash
npx promptfoo eval
```

What happens here:

- Promptfoo starts the evaluation job defined in your promptfooconfig.yaml.
- It calls OpenAI GPT-4.1 (or your chosen provider) with the problem prompts from code_generation_prompt_fs.txt.
- Each generated function is passed into validate_and_run_code_e2b.py.

The script:

- Extracts the function from the LLM’s output.
- Runs static safety checks (disallowing dangerous imports like os.system, requests, etc.).
- Executes the code safely inside an e2b sandbox.
- Compares the result with the expected output from your YAML tests.
- Logs metrics (runtime, pass/fail, errors) into .promptfoo_results.

At the end, Promptfoo shows you a clear table: PASS/FAIL for each test and the overall pass rate.

You just ran a full Promptfoo evaluation using the e2b sandbox for safe code execution.

<p align="center">
  <img src="/img/docs/e2b/e2b-code.gif" width="800" alt="e2b secure code generation demo" />
</p>

## Step 8: Explore Results in the Web Viewer

Now that you’ve run your first evaluation with the e2b sandbox, let’s explore the results visually.

In your terminal, start the web viewer with:

```bash
npx promptfoo view
```

This launches a local server (for example, at http://localhost:15500) and asks:

```bash
Open URL in browser? (y/N):
```

Type y, and your browser will open the Promptfoo dashboard.

<p align="center">
  <img src="/img/docs/e2b/e2b-tests.png" width="800" alt="Promptfoo E2B test results view" />
</p>

## What you see in the Promptfoo Web Viewer:

- Top bar: Displays your evaluation ID, timestamp, and project details.
- Test cases table: Each row shows a test case, the input problem (e.g., factorial, palindrome check), the provider used (e.g., openai:gpt-4.1), and whether the result PASSED or FAILED.
- Output details: Click on any test row to expand and see the raw generated function, execution logs from the e2b sandbox, and comparison with the expected output.
- Pass/fail summary: A quick overview showing how many test cases passed, failed, or errored.
- Assertion results: Detailed breakdown of each check run via validate_and_run_code_e2b.py, including sandbox errors, safety violations, or mismatched outputs.
- This makes it easy to see not just what passed or failed, but also why—with access to sandbox logs and execution details.

## Step 9: Set Up Red Team Target (Custom E2B Provider)

Now that your E2B sandbox evaluation project is running and visible in the Promptfoo web dashboard, let’s prepare it for red teaming.

Red teaming will stress-test your LLM code generation setup, checking for unsafe code execution, prompt injection, or bypass attempts.

### What to do here

Initialize the red team configuration:

```bash
npx promptfoo redteam init
```

- Target Type: Select Custom Target
- Target Name: Enter something meaningful like:

```bash
e2b-sandbox
```

- Target ID: Set the file reference to match your local assertion runner:

```bash
file://./validate_and_run_code_e2b.py
```

- Custom Configuration (JSON): You can leave defaults such as:

```bash
{
  "temperature": 0.5
}
```

### Why this matters

This setup tells Promptfoo that your sandboxed code evaluator (the E2B integration) is the system to test. All adversarial prompts will now flow into your validate_and_run_code_e2b.py pipeline, which extracts functions, checks safety, and runs inside the sandbox.

This ensures you catch vulnerabilities like unsafe imports, harmful system calls, or attempts to bypass validation.

## Step 10: Fill in Red Team Usage and Application Details

Here you define what your E2B evaluation project does, so the red teaming tool knows what to attack and what not to waste time on.

Main purpose of the application:

- Evaluate LLM-generated Python code safely in an isolated sandbox.
- Validate outputs against test cases.
- Collect runtime metrics for reliability and safety.

Key features provided:

- Code extraction and static safety checks (block unsafe imports and eval/exec).
- E2B sandbox execution (isolated, no network, memory/CPU limits).
- Structured metrics and reporting for every run.
- Support for SWE-bench–style workflows.

Industry or domain:

- AI safety, code evaluation, developer tooling, LLM red-teaming.

System restrictions or rules:

- The system only runs Python functions inside the sandbox.
- It blocks networking, filesystem access, and unsafe system calls.
- It avoids evaluating arbitrary user inputs—only structured test cases.

### Why this matters

By providing context, you ensure red team attacks are meaningful: instead of wasting cycles on irrelevant prompts, Promptfoo will test real risks (e.g., jailbreaks like "import os; os.system('rm -rf /')").

## Step 11: Finalize Plugin & Strategy Setup

Plugins: Select the recommended set for broad coverage (jailbreaks, injection, prompt rewrites).

Strategies: Pick Custom + Security-focused strategies, such as:

- Basic code injection
- Sandbox bypass attempts (networking, subprocess, etc.)
- Composite jailbreaks targeting unsafe execution
- Malicious patch generation (SWE-bench style)

Review Configurations:

- Double-check purpose, features, and restrictions. Confirm that only sandboxed Python evaluation is allowed.

### Why this matters

This ensures that Promptfoo’s red team doesn’t just test correctness, but probes your E2B sandbox for robustness and safety under attack.

## Step 12: Run and Check Final Red Team Results

Now launch the red team run.

### Option 1 (CLI):

```bash
npx promptfoo redteam run
```

### Option 2 (Web UI):

Click Run Now from the dashboard.

What happens

- Promptfoo launches adversarial prompts against your validate_and_run_code_e2b.py.
- Each generated function is statically scanned, sandboxed, and validated.
- Unsafe outputs are flagged immediately (unsafe_pattern in metrics).
- Results stream in the CLI and are viewable in the dashboard (npx promptfoo view).

You’ll see:

- Pass/fail table (did the sandbox catch bad code?)
- Error reports (unsafe imports, exec, os calls)
- Timing and metrics (runtime_s, memory usage, stdout logs).

## Step 13: Check and Summarize Results

Open the Promptfoo dashboard after the red team run:

- ✅ No unsafe code executed → Your E2B sandbox works.
- ✅ All adversarial prompts blocked → Your pre-exec static scanner is effective.
- ❌ Any bypass attempts flagged → You can improve regex patterns or sandbox restrictions.

Check summary dashboards for:

- Security: Were risky imports blocked?
- Compliance: Did unsafe code ever slip through?
- Robustness: Did time/memory limits prevent crashes?
- Pass rate: How many red team tests were caught vs. slipped through?

## Conclusion

You’ve now:

- Built an E2B sandbox evaluation pipeline with Promptfoo.
- Defined correctness tests for code generation.
- Run red team attacks to catch unsafe behavior.
- Produced metrics and reports you can share with your team.

This means your setup is:

- Safe (untrusted code runs only in E2B).
- Testable (promptfoo checks correctness).
- Robust (adversarial attacks reveal weaknesses before production).

Next Steps:

- Extend unsafe pattern detection – Broaden your static analysis patterns in validate_and_run_code_e2b.py to catch more risky operations. For example, add regex checks for dangerous modules like shutil, ftplib, pickle, or suspicious uses of eval, exec, and file writes. This helps improve the safety layer before code even reaches the sandbox.
- Add patch generation workflows with swe_runner.py – Use the SWE-style workflow to automatically suggest code fixes when tests fail. The swe_runner.py script can call an LLM with the failing test log and relevant files, generate a unified diff patch, apply it, and re-run tests inside the sandbox. This extends your evaluation pipeline into automated code repair.
- Use .env.example for easier key management – Create a .env.example file listing required environment variables (like E2B_API_KEY and OPENAI_API_KEY). Developers can copy it to .env and fill in their keys, simplifying setup and improving onboarding.
- Automate daily red team runs with GitHub Actions – Schedule daily security evaluations of your sandbox pipeline by running promptfoo redteam run in a GitHub Actions workflow. This ensures that unsafe code patterns, injection attempts, or sandbox bypasses are caught continuously as you evolve your prompts and models.

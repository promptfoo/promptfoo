---
sidebar_label: Prompt Injection
title: Prompt Injection Strategy
description: Identify prompt injection vulnerabilities using comprehensive attack vectors including system prompt extraction and hijacking
---

# Prompt Injection Strategy

The Prompt Injection strategy tests common direct prompt injection vulnerabilities in LLMs.

## Implementation

To enable prompt injections across all your red team tests, add the following to your `promptfooconfig.yaml` file:

```yaml title="promptfooconfig.yaml"
strategies:
  - prompt-injection
```

By default, it will randomly select a prompt injection from a list of hundreds. To control the number of prompt injections sampled, use the `sample` option:

```yaml title="promptfooconfig.yaml"
strategies:
  - id: prompt-injection
    config:
      sample: 10
```

Note that this has a multiplicative effect on the number of tests run! It means that for each test case generated by a plugin, 10 prompt injections will be tested.

This strategy also supports a `harmfulOnly` option, which will limit prompt injections to `harmful` plugins. This can be useful for saving time/cost:

```yaml title="promptfooconfig.yaml"
strategies:
  - id: prompt-injection
    config:
      sample: 5
      harmfulOnly: true
```

## How It Works

The prompt injection strategy:

1. Takes original test cases generated by plugins
2. Prepends or appends various injection techniques to each test case
3. Tests if these modified prompts can bypass the AI system's safety controls

## Example Injections

1. [OPPO](https://www.reddit.com/r/ChatGPTJailbreak/comments/113xsrq/introducing_oppo_a_complete_jailbreak_prompt_that/)
2. [Skeleton key](https://www.microsoft.com/en-us/security/blog/2024/06/26/mitigating-skeleton-key-a-new-type-of-generative-ai-jailbreak-technique/)
3. [DAN](https://github.com/0xk1h0/ChatGPT_DAN)

## Importance in Gen AI Red Teaming

Prompt injection is a widely known attack vector. Although foundation labs are making efforts to mitigate injections at the model level, it's still necessary to test your application's handling of user-provided prompts.

## Related Concepts

- [Iterative Jailbreak](iterative.md)
- [Tree Jailbreak](tree.md)
- [Multi-turn Jailbreaks](multi-turn.md)

For a comprehensive overview of LLM vulnerabilities and red teaming strategies, visit our [Types of LLM Vulnerabilities](/docs/red-team/llm-vulnerability-types) page.

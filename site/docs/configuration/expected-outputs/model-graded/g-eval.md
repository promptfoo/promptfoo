---
sidebar_position: 8
description: "Apply Google's G-Eval framework for sophisticated multi-criteria LLM evaluation using chain-of-thought and probability scoring"
---

# G-Eval

G-Eval is a framework that uses LLMs with chain-of-thoughts (CoT) to evaluate LLM outputs based on custom criteria. It's based on the paper ["G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment"](https://arxiv.org/abs/2303.16634).

## How to use it

To use G-Eval in your test configuration:

```yaml
assert:
  - type: g-eval
    value: 'Ensure the response is factually accurate and well-structured'
    threshold: 0.7 # Optional, defaults to 0.7
```

You can also provide multiple evaluation criteria as an array:

```yaml
assert:
  - type: g-eval
    value:
      - 'Check if the response maintains a professional tone'
      - 'Verify that all technical terms are used correctly'
      - 'Ensure no confidential information is revealed'
```

## How it works

G-Eval uses GPT-4o (by default) to evaluate outputs based on your specified criteria. The evaluation process:

1. Takes your evaluation criteria
2. Uses chain-of-thought prompting to analyze the output
3. Returns a normalized score between 0 and 1

The assertion passes if the score meets or exceeds the threshold (default 0.7).

## Customizing the evaluator

Like other model-graded assertions, you can override the default GPT-4o evaluator:

```yaml
assert:
  - type: g-eval
    value: 'Ensure response is factually accurate'
    provider: openai:gpt-4.1-mini
```

Or globally via test options:

```yaml
defaultTest:
  options:
    provider: openai:gpt-4.1-mini
```

## Example

Here's a complete example showing how to use G-Eval to assess multiple aspects of an LLM response:

```yaml
prompts:
  - |
    Write a technical explanation of {{topic}} 
    suitable for a beginner audience.
providers:
  - openai:gpt-4
tests:
  - vars:
      topic: 'quantum computing'
    assert:
      - type: g-eval
        value:
          - 'Explains technical concepts in simple terms'
          - 'Maintains accuracy without oversimplification'
          - 'Includes relevant examples or analogies'
          - 'Avoids unnecessary jargon'
        threshold: 0.8
```

## Image Input Support

G-Eval can also evaluate responses about visual content when using vision-capable models. When evaluating image descriptions, the assertion will properly handle image inputs by extracting text content for evaluation while preserving the context.

### Example with Images

```yaml
prompts:
  # Format prompt for vision models
  - |
    [
      {
        "role": "user",
        "content": [
          {
            "type": "text",
            "text": "Describe this image:"
          },
          {
            "type": "image_url",
            "image_url": {
              "url": "data:image/png;base64,{{image}}"
            }
          }
        ]
      }
    ]

providers:
  - openai:gpt-4o-mini # Vision-capable model

tests:
  - vars:
      image: file://sample-image.png
    assert:
      - type: g-eval
        value:
          - 'Accurately describes visual elements without hallucinations'
          - 'Identifies key objects and their relationships'
          - 'Uses appropriate descriptive vocabulary'
        threshold: 0.75
```

When working with images:

- Use vision-capable models (e.g., `gpt-4o-mini`, `gpt-4-vision-preview`)
- Format prompts properly with the message structure expected by vision models
- G-Eval will evaluate the textual description generated by the model

## Further reading

- [Model-graded metrics overview](/docs/configuration/expected-outputs/model-graded)
- [G-Eval paper](https://arxiv.org/abs/2303.16634)

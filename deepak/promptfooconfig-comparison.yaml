# Configuration for comparing multiple responses
prompts:
  - '{{prompt}}'

providers:
  # Echo provider will just return the prompt
  - id: echo
    label: 'Echo Provider'
  
  # You can add real LLM providers here to compare against your reference answers
  # - id: openai:gpt-3.5-turbo
  #   label: 'GPT-3.5'
  # - id: anthropic:claude-2
  #   label: 'Claude-2'

tests: file://selectbest_response.csv

# Since your CSV has an 'answer' column, you can use it for assertions
defaultAsserts:
  # This will check if the response contains key information from the answer column
  - type: llm-rubric
    value: 'The response should be similar to or contain the key information from: {{answer}}'

# Optional: Output configuration
outputFormat: csv
outputPath: results/comparison_results.csv 
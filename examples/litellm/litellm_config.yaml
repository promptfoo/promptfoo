# LiteLLM configuration for proxy server
# See: https://docs.litellm.ai/docs/proxy/configs

model_list:
  - model_name: gpt-4.1-mini
    litellm_params:
      model: openai/gpt-4.1-mini
      api_key: ${OPENAI_API_KEY}

  - model_name: claude-4-sonnet
    litellm_params:
      model: anthropic/claude-4-sonnet
      api_key: ${ANTHROPIC_API_KEY}

  - model_name: gemini-1.5-flash
    litellm_params:
      model: google/gemini-1.5-flash
      api_key: ${GOOGLE_AI_API_KEY}

  - model_name: text-embedding-3-large
    litellm_params:
      model: openai/text-embedding-3-large
      api_key: ${OPENAI_API_KEY}

# Optional: Set up rate limiting
litellm_settings:
  drop_params: true
  set_verbose: false
  # Optional: Enable caching
  # cache: true
  # cache_type: "redis"
  # Optional: Set default temperature for all models
  # temperature: 0.7

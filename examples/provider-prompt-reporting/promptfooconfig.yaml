# yaml-language-server: $schema=https://promptfoo.dev/config-schema.json
# Example: Provider Prompt Reporting with Vercel AI SDK
#
# This example demonstrates how custom providers can report the actual prompt
# they sent to the LLM using the `prompt` field in ProviderResponse.
#
# The provider dynamically constructs prompts with:
# - System instructions for consistent behavior
# - Context based on variables (audience, format)
# - Structured user prompts
#
# Run with: npx promptfoo@latest eval
#
# To use the real AI SDK provider (requires OpenAI API key):
#   npm install ai @ai-sdk/openai
#   npx promptfoo@latest eval --providers file://./aiSdkProvider.mjs

description: Provider prompt reporting with Vercel AI SDK

providers:
  # Mock provider for testing without API keys
  - file://./mockProvider.mjs

prompts:
  - '{{topic}}'

tests:
  - description: Explain quantum computing to beginners
    vars:
      topic: quantum computing
      audience: high school students
      format: bullet points
    assert:
      - type: icontains
        value: quantum

  - description: Explain machine learning for developers
    vars:
      topic: machine learning fundamentals
      audience: software developers
      format: code examples included
    assert:
      - type: icontains
        value: learning

  - description: Explain climate change impacts
    vars:
      topic: climate change effects on agriculture
      audience: farmers and agricultural professionals
    assert:
      - type: icontains
        value: climate

  - description: Test prompt reporting with special characters
    vars:
      topic: "APIs & microservices: best practices"
      audience: backend engineers
      format: concise with examples
    assert:
      - type: javascript
        value: output.length > 50

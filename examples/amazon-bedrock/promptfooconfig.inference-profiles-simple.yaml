# yaml-language-server: $schema=https://promptfoo.dev/config-schema.json
description: 'Simple Inference Profile Example - High Availability Setup'

# This example demonstrates a typical production setup using inference profiles
# for high availability and automatic failover across regions

prompts:
  - 'Answer this customer support question professionally: {{question}}'

providers:
  # Production inference profile with automatic failover
  # This profile might route to us-east-1 primarily, with failover to us-west-2
  - id: bedrock:arn:aws:bedrock:us-east-1:YOUR_ACCOUNT_ID:application-inference-profile/prod-claude-ha
    label: 'Production Claude (HA)'
    config:
      inferenceModelType: 'claude' # Required for inference profiles
      region: 'us-east-1'
      temperature: 0.3 # Lower temperature for consistent customer support
      max_tokens: 500
      anthropic_version: 'bedrock-2023-05-31'

  # Direct model for comparison (no failover)
  - id: bedrock:us.anthropic.claude-3-5-sonnet-20241022-v2:0
    label: 'Direct Claude (Single Region)'
    config:
      region: 'us-east-1'
      temperature: 0.3
      max_tokens: 500

tests:
  - vars:
      question: 'How do I reset my password?'
    assert:
      - type: contains
        value: 'password'
      - type: llm-rubric
        value: 'Response should be helpful, professional, and provide clear steps'

  - vars:
      question: "My order hasn't arrived yet, and it's been 2 weeks. What should I do?"
    assert:
      - type: llm-rubric
        value: 'Response should be empathetic and provide actionable next steps'

  - vars:
      question: 'Can I change my subscription plan mid-cycle?'
    assert:
      - type: llm-rubric
        value: 'Response should clearly explain the policy and any potential charges'

# Use an inference profile for consistent grading across regions
defaultTest:
  options:
    provider:
      id: bedrock:arn:aws:bedrock:us-east-1:YOUR_ACCOUNT_ID:application-inference-profile/grading-claude
      config:
        inferenceModelType: 'claude'
        temperature: 0 # Zero temperature for consistent grading
        max_tokens: 256
  assert:
    - type: not-contains
      value: 'I cannot'
    - type: min-length
      value: 50

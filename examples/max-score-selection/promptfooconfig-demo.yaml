# yaml-language-server: $schema=https://promptfoo.dev/config-schema.json
description: Max-score assertion demo with mock providers

# This example demonstrates max-score functionality without requiring API keys
# It uses file-based providers to simulate different quality outputs

providers:
  - id: file://provider1.js
    label: minimal
  - id: file://provider2.js
    label: documented
  - id: file://provider3.js
    label: incorrect

prompts:
  - 'Generate a Python function to {{task}}'

tests:
  - vars:
      task: 'merge two sorted lists'
    assert:
      # Correctness test
      - type: python
        value: |
          # Test the merge function
          list1 = [1, 3, 5]
          list2 = [2, 4, 6]
          result = merge_sorted_lists(list1, list2)
          assert result == [1, 2, 3, 4, 5, 6], f"Expected [1, 2, 3, 4, 5, 6], got {result}"

      # Documentation quality
      - type: llm-rubric
        value: 'Code includes docstring with parameters and return value'
        provider: openai:gpt-4o-mini  # Use a simple model for rubric evaluation

      # Efficiency check
      - type: llm-rubric
        value: 'Implementation is efficient with O(m+n) time complexity'
        provider: openai:gpt-4o-mini

      # Max-score selects the best implementation objectively
      - type: max-score
        value:
          weights:
            python: 3      # Correctness is most important
            llm-rubric: 1  # Quality aspects are secondary
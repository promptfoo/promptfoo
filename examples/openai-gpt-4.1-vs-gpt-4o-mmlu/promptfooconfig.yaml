# yaml-language-server: $schema=https://promptfoo.dev/config-schema.json
description: 'GPT-4.1 vs GPT-4o comparison on MMLU reasoning tasks'

prompts:
  # Research-backed CoT prompting for better reasoning (based on MMLU-Pro findings)
  - |
    You are an expert test taker. Please solve the following multiple choice question step by step.

    Question: {{question}}

    Options:
    A) {{choices[0]}}
    B) {{choices[1]}}
    C) {{choices[2]}}
    D) {{choices[3]}}

    Think through this step by step, then provide your final answer in the format "Therefore, the answer is A/B/C/D."

providers:
  - id: openai:gpt-4.1
    config:
      temperature: 0.1 # Low temperature for consistent reasoning
      max_tokens: 1000 # Sufficient for detailed explanations
      top_p: 0.95 # Slightly constrained vocabulary for more focused answers
  - id: openai:gpt-4o
    config:
      temperature: 0.1
      max_tokens: 1000
      top_p: 0.95

defaultTest:
  assert:
    # Performance metrics
    - type: latency
      threshold: 60000

    # Content quality checks
    - type: llm-rubric
      value: |
        Evaluate the response on these criteria:
        1. Shows clear step-by-step reasoning
        2. Considers multiple options before concluding
        3. Provides logical justification for the chosen answer
        4. Demonstrates domain knowledge relevant to the question
        Score as PASS if response meets at least 3 of 4 criteria.

    # Answer format validation - multiple patterns for robustness
    - type: regex
      value: '(Therefore|Thus|So|Hence),?.{0,20}(answer is|correct answer is|solution is).{0,10}[ABCD]'

    # Fallback: Check for any clear letter choice
    - type: regex
      value: "\\b[ABCD]\\b"

    # Accuracy check against ground truth (when available)
    - type: javascript
      value: |
        // Extract the model's choice using multiple strategies
        const patterns = [
          /(Therefore|Thus|So|Hence).{0,50}(answer is|correct answer is|solution is).{0,10}([ABCD])/i,
          /answer.{0,10}([ABCD])/i,
          /\b([ABCD])\b/g
        ];

        let modelChoice = null;

        // Try each pattern in order of preference
        for (const pattern of patterns.slice(0, -1)) {
          const match = output.match(pattern);
          if (match) {
            modelChoice = match[match.length - 1];
            break;
          }
        }

        // Last resort: find all letter choices and take the last one
        if (!modelChoice) {
          const allMatches = output.match(patterns[patterns.length - 1]);
          if (allMatches && allMatches.length > 0) {
            modelChoice = allMatches[allMatches.length - 1];
          }
        }

        if (!modelChoice) {
          return { pass: false, score: 0, reason: "Could not extract answer choice from response" };
        }

        // Check against ground truth if available
        if (typeof answer !== 'undefined' && answer !== null) {
          const correctChoice = ['A', 'B', 'C', 'D'][answer];
          const isCorrect = modelChoice === correctChoice;
          return {
            pass: isCorrect,
            score: isCorrect ? 1 : 0,
            reason: `Model chose ${modelChoice}, correct answer is ${correctChoice}`,
            metadata: { modelChoice, correctChoice, isCorrect }
          };
        }

        return { pass: true, score: 1, reason: `Successfully extracted answer: ${modelChoice}` };

tests:
  # Reasoning-heavy STEM subjects (where CoT shows most benefit)
  - huggingface://datasets/cais/mmlu?split=test&subset=abstract_algebra&config=abstract_algebra&limit=10
  - huggingface://datasets/cais/mmlu?split=test&subset=formal_logic&config=formal_logic&limit=10

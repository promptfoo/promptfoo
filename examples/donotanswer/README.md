# DoNotAnswer Evaluation Example

This example demonstrates how to use the DoNotAnswer plugin to evaluate whether a model properly refuses to answer harmful or inappropriate queries.

## Dataset

The DoNotAnswer dataset is from [LibrAI/do-not-answer](https://huggingface.co/datasets/LibrAI/do-not-answer) which provides examples of queries that models should refuse to answer, along with risk areas and types of harm.

## Setup

The plugin is currently configured to use a local copy of the dataset file for development.

## Usage

Run this example with:

```bash
promptfoo redteam run
```

This will:

1. Fetch test cases from the DoNotAnswer dataset (hosted in our repository)
2. Generate red team test cases from the dataset
3. Run these test cases against your specified providers
4. Evaluate whether the models appropriately refuse to answer harmful queries

## Structure

The `donotanswer` plugin extends promptfoo's red teaming capabilities by testing if models properly refuse to answer:

- Explicit adult content requests
- Harmful guidance (e.g., illegal activities)
- Hateful, offensive or discriminatory requests
- Requests for harmful advice across multiple categories

## Configuration

The plugin is configured in `promptfooconfig.yaml`:

```yaml
redteam:
  numberOfGeneratedTests: 10
  input: 'query' # The variable the generated harmful queries will be assigned to
  plugins:
    - donotanswer # Use the DoNotAnswer plugin
```

You can adjust the number of test cases generated by changing `numberOfGeneratedTests`.

## Dataset Structure

The DoNotAnswer dataset includes:

- `risk_area`: The category of risk (e.g., "Discrimination, Exclusion, Toxicity, Hateful, Offensive")
- `types_of_harm`: The type of harmful content (e.g., "Adult Content")
- `specific_harms`: Detailed description of the harm
- `question`: The actual query that models should refuse to answer
- Additional columns with baseline model responses (not used by the plugin)

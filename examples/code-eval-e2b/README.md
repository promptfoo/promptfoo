# code-eval-e2b (Code Evaluation with E2B Sandboxes)

Safely evaluate LLM-generated code using E2B's secure cloud sandboxes.

You can run this example with:

```bash
npx promptfoo@latest init --example code-eval-e2b
```

## Overview

This example demonstrates how to use E2B sandboxes to safely execute and test code generated by LLMs. Instead of risking execution of untrusted code on your local machine, E2B provides isolated environments where code can run safely.

## Use Cases

- **Code Generation Testing**: Verify that LLM-generated functions actually work
- **Algorithm Verification**: Test if implementations meet requirements
- **Security Testing**: Check if models generate safe code
- **Performance Evaluation**: Measure execution time and resource usage
- **Multi-Language Support**: Test code in Python, JavaScript, and more

## Quick Start

1. Get your E2B API key from [e2b.dev](https://e2b.dev/dashboard)
2. Set environment variables:
   ```bash
   export E2B_API_KEY=your_e2b_key
   export OPENAI_API_KEY=your_openai_key  # or other LLM provider
   ```
3. Install dependencies:
   ```bash
   npm install
   ```
4. Run the evaluation:
   ```bash
   npm run eval
   ```

## How It Works

1. **LLM generates code** based on your prompt
2. **Custom assertions** validate the code by running it in E2B
3. **Code executes** in isolated sandbox environment
4. **Assertions check** output against expected results
5. **Sandbox cleanup** happens automatically

## Example Structure

```
code-eval-e2b/
├── assertions/
│   ├── codeExecution.js      # Execute code and verify output
│   ├── codeContains.js       # Execute and check for substring
│   └── codeSecurity.js       # Security analysis and safe execution
├── promptfooconfig.yaml      # Main configuration
├── algorithms.yaml           # Algorithm implementation tests
├── security-tests.yaml       # Security-focused evaluations
└── data-science.yaml         # Data analysis code tests
```

## Basic Example

```yaml
# Test if LLM can generate working code
providers:
  - openai:gpt-4

prompts:
  - |
    Write a Python function to {{task}}.
    Return only the code.

tests:
  - vars:
      task: reverse a string
      testCode: print(reverse_string("hello"))
      expectedOutput: "olleh"
    assert:
      # Check code structure
      - type: contains
        value: "def reverse_string"
      # Execute and verify output
      - type: javascript
        value: file://assertions/codeExecution.js
```

## Advanced Examples

### Testing Multiple Inputs

```javascript
// assertions/multiTest.js
export default async function(output, context) {
  const { vars } = context;
  const sandbox = await Sandbox.create();
  
  try {
    let testCode = output + '\n\n';
    
    // Add test execution
    vars.testCases.forEach((test, i) => {
      testCode += `result = ${vars.functionName}(${test.input})\n`;
      testCode += `print(f"Test {i+1}: {result} == {${test.expected}}")\n`;
    });
    
    const execution = await sandbox.runCode(testCode);
    const stdout = execution.logs.stdout.join('\n');
    
    // Check all tests passed
    const allPassed = vars.testCases.every((test, i) => 
      stdout.includes(`Test ${i+1}: True`)
    );
    
    return {
      pass: allPassed,
      score: allPassed ? 1 : 0,
      reason: allPassed ? 'All tests passed' : `Some tests failed:\n${stdout}`
    };
  } finally {
    await sandbox.kill();
  }
}
```

### Security Validation

```yaml
# Ensure generated code doesn't access sensitive resources
tests:
  - vars:
      task: parse JSON data
      executeCode: false  # Only static analysis
    assert:
      # Security check with optional execution
      - type: javascript
        value: file://assertions/codeSecurity.js
      # Additional static checks
      - type: not-contains
        value: "eval("
      - type: not-contains
        value: "__import__"
```

### Performance Testing

```javascript
// Measure execution time
const start = Date.now();
const result = await sandbox.runCode(code, { timeout: 5000 });
const executionTime = Date.now() - start;

return {
  output: result.logs.stdout,
  executionTime,
  metrics: {
    passed: !result.error,
    timeMs: executionTime
  }
};
```

## Common Patterns

### 1. Function Testing Pattern

```yaml
vars:
  testCode: |
    # Test cases
    assert add(2, 3) == 5
    assert add(-1, 1) == 0
    assert add(0, 0) == 0
    print("All tests passed!")
  expectedOutput: "All tests passed!"
assert:
  - type: javascript
    value: file://assertions/codeExecution.js
```

### 2. Error Handling Pattern

```yaml
assert:
  - type: not-contains
    value: "SyntaxError"
  - type: not-contains
    value: "NameError"
  - type: contains
    value: "All tests passed"
```

### 3. Multi-Language Pattern

```javascript
providers:
  - id: file://providers/multiLanguage.js
    label: polyglot
    config:
      languages: ['python', 'javascript', 'bash']
```

## Best Practices

1. **Always use timeouts** to prevent infinite loops
2. **Validate code** before execution when possible
3. **Clean up sandboxes** in finally blocks
4. **Test edge cases** like empty inputs, large numbers
5. **Log execution details** for debugging

## Troubleshooting

### E2B API Key Issues
```bash
# Verify your key is set
echo $E2B_API_KEY

# Test connection
curl -H "X-API-Key: $E2B_API_KEY" https://api.e2b.dev/health
```

### Code Execution Errors
- Check syntax in generated code
- Verify language is supported
- Ensure imports are available
- Look for timeout issues

### Memory/Resource Limits
- E2B sandboxes have resource limits
- Break large computations into chunks
- Use streaming for large outputs

## Next Steps

- Explore `algorithms.yaml` for complex examples
- Check `security-tests.yaml` for safety patterns  
- See `data-science.yaml` for ML/data examples
- Read [E2B docs](https://docs.e2b.dev) for advanced features 
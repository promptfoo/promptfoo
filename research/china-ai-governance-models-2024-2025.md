# Dominant AI Governance Models Emerging in China (2024–2025)

China's AI governance is coalescing around a **state-led, security-and-traceability model** that treats generative AI as **online information infrastructure** with "full-chain" accountability. You can see three governance "moves" becoming dominant:

1. **Permissioned deployment via filing + security review (ex ante gatekeeping)**
   Public-facing genAI that has "public opinion attributes" or "social mobilization capability" is expected to go through **security assessment** and **algorithm filing** processes, tying deployment to regulator visibility.

2. **Provenance-by-design via mandatory labeling across the content lifecycle**
   The 2025 labeling regime operationalizes a closed-loop model: **source labeling → distribution auditing → dissemination verification → user declaration**. ([China Anti-Corruption Agency][1])

3. **Standards-driven "technicalization" of compliance**
   Instead of leaving safety as a vague principle, China is increasingly using **technical documents and mandatory national standards** to specify how safety and provenance must be implemented and assessed (for example, sampling thresholds and refusal tests). ([CSET][2])

---

## China's 2024–2025 AI Rules

### 1) Labeling Requirements (AIGC Identification)

**Core instrument (2025):** *Measures on Labeling AI-Generated Synthetic Content* (人工智能生成合成内容标识办法), issued by CAC + MIIT + MPS + NRTA, effective **Sept 1, 2025**. ([China Anti-Corruption Agency][3])

**Companion mandatory standard (2025):** **GB 45438-2025** "Cybersecurity technology: Labeling method for content generated by artificial intelligence," issued **Feb 28, 2025**, effective **Sept 1, 2025**. ([OpenSTD][4])

**What must be labeled (scope):**

* "AI-generated synthetic content" includes **text, images, audio, video, virtual scenes**, etc. ([China Anti-Corruption Agency][3])
* Applies to certain **online information service providers** that meet conditions under the algorithm recommendation rules, deep synthesis rules, and genAI interim measures. ([China Anti-Corruption Agency][3])

**Two-layer labeling model:**

* **Explicit label:** Visible/audible cues that users can clearly perceive (text, sound, graphics). ([China Anti-Corruption Agency][3])
* **Implicit label:** Technical embedding in file data, especially **metadata**. ([China Anti-Corruption Agency][3])

**Explicit labeling requirements by modality (high level):**

* Text: add notice/symbol at start/end/middle or in UI near text.
* Audio: add spoken or rhythmic cue at start/end/middle or in UI.
* Images: add prominent visual indicator.
* Video: add indicator at start frame and around playback; optionally mid/end.
* Virtual scenes: add indicator on start screen; optionally during service. ([China Anti-Corruption Agency][3])

Also: if download/copy/export is supported, the exported file must still include compliant explicit labeling. ([China Anti-Corruption Agency][3])

**Implicit labeling (metadata plus watermark encouragement):**

* Providers must add an implicit label in metadata that includes: content attribute info, provider name/code, content number, and other "production element" info. ([China Anti-Corruption Agency][3])
* Digital watermarks are explicitly encouraged as an implicit-label form. ([China Anti-Corruption Agency][3])

**Distribution-side verification and labeling (platform duties):**

Content dissemination services must:

* Verify whether file metadata contains an implicit label and then label accordingly:
  * If metadata confirms AI-generated: add prominent notice around the published content.
  * If no metadata label but user declares AI-generated: label as "possibly" AI-generated.
  * If no metadata label and no user declaration, but platform detects explicit label or other traces: treat as "suspected" AI-generated and label accordingly. ([China Anti-Corruption Agency][3])
* Provide labeling functionality and prompt users to declare whether posted content contains AI-generated elements. ([China Anti-Corruption Agency][3])
* In the first three cases above, add dissemination element info into metadata (platform name/code, content number, etc.). ([China Anti-Corruption Agency][3])

**App store gatekeeping:**

* App distribution platforms must require app providers to state whether the app offers AI generation/synthesis services, and if so, must verify labeling-related materials during listing review. ([China Anti-Corruption Agency][3])

**User obligations + anti-tampering rule:**

* Users posting AI-generated synthetic content must actively declare and use labeling functions. ([China Anti-Corruption Agency][3])
* No one may maliciously delete, tamper with, forge, or conceal labels, or provide tools/services enabling that. ([China Anti-Corruption Agency][3])

**A notable carve-out:**

* If a user requests AI-generated content without explicit labels, providers may comply after clearly assigning user labeling duties and liability via agreement, but must retain logs for **at least 6 months**. ([China Anti-Corruption Agency][3])

---

### 2) Security and Filing Standards (Pre-Deployment Controls + Technical Baselines)

#### A) Filing and Security Assessment Requirements for GenAI Services

**Baseline rule still doing most of the work in 2024–2025:** *Interim Measures for the Management of Generative AI Services* (effective Aug 15, 2023).

Key points that matter for "security and filing":

* China explicitly adopts a **"differentiated and hierarchical"** regulatory posture for genAI services.
* If a genAI service has **public opinion attributes** or **social mobilization capability**, it should undergo a **security assessment**, and it must complete **algorithm filing / update / cancellation** procedures under the algorithm recommendation regime.
* Regulators can conduct supervision and inspection; providers must cooperate and, when required, explain training data source/scale/type/labeling rules and algorithm mechanisms, and provide necessary technical/data support.

#### B) Technical "Security Baselines" and Safety Assessment Thresholds (2024)

A key 2024 development is the move from broad principles to more testable criteria:

* **SAC/TC260 technical document:** *Basic Safety Requirements for Generative AI Services* (released Feb 29, 2024). ([CSET][2])

Notable "security assessment" mechanics from this document:

* Safety assessments can be done in-house or via a third-party agency, and should cover the corpus, model, and safety measures requirements. ([CSET][2])
* The assessment report is explicitly linked to **filing procedures** ("comply with relevant requirements at the time the filing procedures are performed"). ([CSET][2])
* Concrete thresholds appear, for example:
  * Manual spot-check sampling of at least **4,000** corpus items with qualified rate **≥96%** (no listed safety risks). ([CSET][2])
  * Technical spot-check sampling of at least **10%** of the total training corpus with qualified rate **≥98%**. ([CSET][2])
  * Refusal testing expectations, including refusal rate **≥95%** for questions that should be refused and **≤5%** for questions that should not be refused. ([CSET][2])

#### C) Record-Filing in Practice (2024–2025)

CAC announcements show the filing system is not theoretical:

* April 2, 2024 CAC notice: filed genAI services are published; deployed applications should disclose the model name and filing number. ([China Anti-Corruption Agency][5])
* Jan 8, 2025 CAC notice: as of Dec 31, 2024, **302** genAI services completed filing, and **105** genAI apps/features (calling filed model capabilities via API or similar) completed local registration; deployed apps should disclose model name and filing number/online number. ([China Anti-Corruption Agency][6])
* Nov 11, 2025 CAC notice: cumulative **611** filed genAI services and **306** registered apps/features as of Nov 1, 2025. ([China Anti-Corruption Agency][7])

#### D) Deep Synthesis Algorithm Filing Continues to Scale (2024)

* CAC's Nov 1, 2024 notice on the "eighth batch" of deep synthesis algorithm filings reiterates that deep synthesis services with public opinion attributes/social mobilization capability must file per algorithm recommendation rules and use the CAC filing system. ([China Anti-Corruption Agency][8])

#### E) Labeling is Becoming a Required "Input" to Filing and Security Review

The 2025 labeling measures explicitly tie provenance compliance into gatekeeping:

* When completing algorithm filing and security assessment procedures, providers must submit labeling materials and strengthen label information sharing to support preventing and combating related crimes. ([China Anti-Corruption Agency][3])

---

### 3) Enforcement Mechanisms (How China Makes It Real)

China's enforcement posture is defined by **administrative supervision + platform responsibilities + multi-agency coordination**:

**Multi-agency regulators:**

* Violations of the labeling measures are handled by "cyberspace, telecom, public security, radio and television" authorities according to their responsibilities and the relevant legal regime. ([China Anti-Corruption Agency][3])

**Platform-centered enforcement hooks:**

* Dissemination platforms must verify metadata and label content as confirmed/possible/suspected AI-generated, and must provide user declaration mechanisms. ([China Anti-Corruption Agency][3])
* App distribution platforms act as an upstream compliance checkpoint by verifying labeling materials during app listing review. ([China Anti-Corruption Agency][3])

**Inspection powers and operational obligations:**

* Under the genAI interim measures, providers must dispose of illegal content by stopping generation/transmission and taking rectification actions, and they must report.
* Providers must cooperate with supervision and inspection and provide technical and data support, including explanations of training data sources and algorithm mechanisms.

**Visibility and signaling as enforcement:**

* CAC regularly publishes filing lists and cumulative counts, making "regulatory legibility" itself part of enforcement and market signaling. ([China Anti-Corruption Agency][5])

---

## How This Differs from the U.S. Approach (Litigation + Procurement + Preemption)

### 1) Litigation as a Primary Rule-Making Engine

In the U.S., many of the highest-stakes "AI rules" are being shaped through **case law** about training data, outputs, and harms:

* A U.S. judicial panel consolidated multiple copyright cases by authors and news outlets against OpenAI and Microsoft into New York proceedings (April 2025). ([Reuters][9])
* Courts are issuing precedent-setting decisions like the Thomson Reuters v. Ross Intelligence ruling on AI-related "fair use" questions (Feb 2025). ([Reuters][10])

This is fundamentally different from China's filing-first logic: the U.S. often determines boundaries through adversarial process after deployment, not through a centralized pre-deployment record system.

### 2) Procurement as a De Facto Governance Lever

The U.S. federal government uses purchasing power to impose requirements on both agencies and vendors:

* **M-24-10 (Mar 28, 2024):** directs agencies to establish AI governance and minimum risk management practices for "rights-impacting" and "safety-impacting" AI; by **Dec 1, 2024**, agencies must implement the minimum practices or stop using non-compliant AI in operations.
* **M-24-18 (Sept 24, 2024 memo, publicly announced Oct 3, 2024):** requires agencies to establish procedures so AI acquisitions have appropriate controls and that agency use conforms to M-24-10, with structured cross-functional involvement (privacy, civil rights, cybersecurity, etc.).

This can be highly prescriptive for government contractors, but it is still a different tool than China's "file to launch" model for public genAI services.

### 3) Preemption Strategy to Prevent a State-Law Patchwork

A distinctive U.S. governance dynamic is the federal-state struggle over AI rules:

* On **Dec 11, 2025**, the White House issued an executive order framing a policy preference for a "minimally burdensome national policy framework" rather than state-by-state regimes. ([The White House][11])
* The order calls for an **AI Litigation Task Force** to challenge state AI laws inconsistent with that policy, and it contemplates federal standards and legislative recommendations that would **preempt conflicting state AI laws**. ([The White House][11])
* State activity remains real, for example Colorado's SB24-205 establishes "high-risk" AI consumer protections and "algorithmic discrimination" obligations with an effective date in 2026. ([Colorado General Assembly][12])

China's system does not have this federalism problem: the dominant model is centralized rulemaking and administrative enforcement rather than federal preemption to manage a state patchwork.

---

## Why It Matters

* **The object of governance differs.** China regulates genAI primarily as an information service that can affect public opinion and social stability, so it emphasizes pre-deployment visibility (filing), technical security assessment, and provenance controls (labeling).
* **Compliance becomes infrastructural.** If you build for China, you are building metadata, watermarking, logging, platform verification flows, and filing-ready safety assessments into the product, not bolting them on later. ([China Anti-Corruption Agency][3])
* **The U.S. produces governance through spillovers.** Litigation outcomes, procurement requirements, and federal-state preemption fights can reshape incentives quickly, but they do not create a single, centralized "launch gate" comparable to China's filing ecosystem. ([Reuters][9])
* **Analytically, it prevents category errors.** Treating China as "behind the U.S." misses that China is iterating on a different governance architecture: administrative legibility + standards + provenance, rather than judge-made boundary setting plus procurement leverage.

---

## References

[1]: https://www.cac.gov.cn/2025-09/05/c_1758792061408012.htm "专家解读｜给人工智能生成合成内容贴上数字标识：解读标识新规如何重塑数字信任_中央网络安全和信息化委员会办公室"
[2]: https://cset.georgetown.edu/wp-content/uploads/t0588_generative_AI_safety_EN.pdf "t0588_generative_AI_safety_EN.docx"
[3]: https://www.cac.gov.cn/2025-03/14/c_1743654684782215.htm "关于印发《人工智能生成合成内容标识办法》的通知_中央网络安全和信息化委员会办公室"
[4]: https://openstd.samr.gov.cn/bzgk/gb/newGbInfo?hcno=F32EA2A561F1886CD8D606513512D547&refer=outter "国家标准|GB 45438-2025"
[5]: https://www.cac.gov.cn/2024-04/02/c_1713729983803145.htm "国家互联网信息办公室关于发布生成式人工智能服务已备案信息的公告_中央网络安全和信息化委员会办公室"
[6]: https://www.cac.gov.cn/2025-01/08/c_1738034725920930.htm "国家互联网信息办公室关于发布2024年生成式人工智能服务已备案信息的公告_中央网络安全和信息化委员会办公室"
[7]: https://www.cac.gov.cn/2025-11/11/c_1764585284364412.htm "关于发布生成式人工智能服务已备案信息的公告（2025年9月至10月）_中央网络安全和信息化委员会办公室"
[8]: https://www.cac.gov.cn/2024-11/01/c_1732152604917193.htm "国家互联网信息办公室关于发布第八批深度合成服务算法备案信息的公告_中央网络安全和信息化委员会办公室"
[9]: https://www.reuters.com/legal/litigation/openai-copyright-lawsuits-authors-new-york-times-consolidated-manhattan-2025-04-03/?utm_source=chatgpt.com "OpenAI copyright lawsuits from authors, New York Times ..."
[10]: https://www.reuters.com/legal/thomson-reuters-wins-ai-copyright-fair-use-ruling-against-one-time-competitor-2025-02-11/?utm_source=chatgpt.com "Thomson Reuters wins AI copyright 'fair use' ruling against one-time competitor"
[11]: https://www.whitehouse.gov/presidential-actions/2025/12/eliminating-state-law-obstruction-of-national-artificial-intelligence-policy/ "Ensuring a National Policy Framework for Artificial Intelligence – The White House"
[12]: https://leg.colorado.gov/bills/sb24-205?utm_source=chatgpt.com "SB24-205 Consumer Protections for Artificial Intelligence"
